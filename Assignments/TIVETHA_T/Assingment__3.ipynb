{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "cXcDmgIVDP1e"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 16"
      ],
      "metadata": {
        "id": "kdBKXDq0D3dx"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_augmentation = Sequential(\n",
        "  [\n",
        "    layers.RandomFlip(\"horizontal\",input_shape=(180, 180, 3)),\n",
        "    layers.RandomRotation(0.1),\n",
        "    layers.RandomZoom(0.1),\n",
        "  ]\n",
        ")"
      ],
      "metadata": {
        "id": "jeI0M2i-D8m9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_set = tf.keras.utils.image_dataset_from_directory(\n",
        "  \"flowers\",\n",
        "  validation_split=0.25,\n",
        "  subset=\"training\",\n",
        "  seed=132,\n",
        "  image_size=(180, 180),\n",
        "  batch_size=batch_size)"
      ],
      "metadata": {
        "id": "-4t5O9bUEISH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_data_set = tf.keras.utils.image_dataset_from_directory(\n",
        "  \"flowers\",\n",
        "  validation_split=0.25,\n",
        "  subset=\"validation\",\n",
        "  seed=132,\n",
        "  image_size=(180, 180),\n",
        "  batch_size=batch_size)"
      ],
      "metadata": {
        "id": "KNPLBZHREQ2a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = train_data_set.class_names"
      ],
      "metadata": {
        "id": "0wtvCHpQESO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15, 15))\n",
        "for images, labels in train_data_set.take(1):\n",
        "  for i in range(6):\n",
        "    ax = plt.subplot(3, 3, i + 1)\n",
        "    plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
        "    plt.title(class_names[labels[i]])"
      ],
      "metadata": {
        "id": "h7adCQTlEWSZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normalization_layer = layers.Rescaling(1./255)"
      ],
      "metadata": {
        "id": "hY5vNgwzEZG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_normalized = train_data_set.map(lambda x, y: (normalization_layer(x), y))\n",
        "image_batch, labels_batch = next(iter(dataset_normalized))\n",
        "first_image = image_batch[0]\n",
        "print(np.min(first_image), np.max(first_image))"
      ],
      "metadata": {
        "id": "tzxyG6mCEegj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = len(class_names)\n",
        "\n",
        "model = Sequential([\n",
        "  data_augmentation,\n",
        "  layers.Rescaling(1./255, input_shape=(180, 180, 3)),\n",
        "  # adding convolutional layer\n",
        "  layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
        "  # adding maxpooling layer\n",
        "  layers.MaxPooling2D(),\n",
        "  layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
        "  layers.MaxPooling2D(),\n",
        "  layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
        "  layers.MaxPooling2D(),\n",
        "  # adding flatten\n",
        "  layers.Flatten(),\n",
        "  # adding dense hidden layer\n",
        "  layers.Dense(128, activation='relu'),\n",
        "  # adding dense output layer\n",
        "  layers.Dense(num_classes)\n",
        "])"
      ],
      "metadata": {
        "id": "x4qgb92AEjNH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compiling model with categorical cross entropy and adam optimizer\n",
        "model.compile(optimizer='adam',\n",
        "loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "v6TdEFpZEjkD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs=15\n",
        "history = model.fit(train_data_set,validation_data=val_data_set,epochs=epochs)"
      ],
      "metadata": {
        "id": "zNNTd8MHEq38"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs_range = range(epochs)\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.plot(epochs_range, history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(epochs_range, history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7IqmvG-aEu2I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 8))\n",
        "plt.plot(epochs_range, history.history['loss'], label='Training Loss')\n",
        "plt.plot(epochs_range, history.history['val_loss'], label='Validation Loss')\n",
        "plt.legend()\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TtVJ9iDXEzKz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"CNN Model for Classification Of Flowers.h5\")"
      ],
      "metadata": {
        "id": "GWMg4jLMIdcZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights('CNN Model for Classification Of Flowers.h5')"
      ],
      "metadata": {
        "id": "5vjrVNK-Iiic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sunflower_url = \"https://storage.googleapis.com/download.tensorflow.org/example_images/592px-Red_sunflower.jpg\"\n",
        "sunflower_path = tf.keras.utils.get_file('Red_sunflower', origin=sunflower_url)\n",
        "\n",
        "img = tf.keras.utils.load_img(\n",
        "    sunflower_path, target_size=(180, 180)\n",
        ")\n",
        "img_array = tf.keras.utils.img_to_array(img)\n",
        "img_array = tf.expand_dims(img_array, 0) # Create a batch\n",
        "\n",
        "predictions = model.predict(img_array)\n",
        "score = tf.nn.softmax(predictions[0])\n",
        "\n",
        "print(class_names[np.argmax(score)],100 * np.max(score))"
      ],
      "metadata": {
        "id": "Bs4DroiRIm7r"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}